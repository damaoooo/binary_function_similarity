{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64143f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                                                                            #\n",
    "#  Code for the USENIX Security '22 paper:                                   #\n",
    "#  How Machine Learning Is Solving the Binary Function Similarity Problem.   #\n",
    "#                                                                            #\n",
    "#  MIT License                                                               #\n",
    "#                                                                            #\n",
    "#  Copyright (c) 2019-2022 Cisco Talos                                       #\n",
    "#                                                                            #\n",
    "#  Permission is hereby granted, free of charge, to any person obtaining     #\n",
    "#  a copy of this software and associated documentation files (the           #\n",
    "#  \"Software\"), to deal in the Software without restriction, including       #\n",
    "#  without limitation the rights to use, copy, modify, merge, publish,       #\n",
    "#  distribute, sublicense, and/or sell copies of the Software, and to        #\n",
    "#  permit persons to whom the Software is furnished to do so, subject to     #\n",
    "#  the following conditions:                                                 #\n",
    "#                                                                            #\n",
    "#  The above copyright notice and this permission notice shall be            #\n",
    "#  included in all copies or substantial portions of the Software.           #\n",
    "#                                                                            #\n",
    "#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,           #\n",
    "#  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF        #\n",
    "#  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND                     #\n",
    "#  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE    #\n",
    "#  LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION    #\n",
    "#  OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION     #\n",
    "#  WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.           #\n",
    "#                                                                            #\n",
    "#  Dataset-2 creation                                                        #\n",
    "#                                                                            #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "594087b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "\n",
    "pdcsv = lambda x: pd.read_csv(x, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7625c",
   "metadata": {},
   "source": [
    "The following table summarizes the criteria used to generate positive pairs for each task:\n",
    "* The `X` indicates that the variable is required to be different in each pair\n",
    "* The `*` indicates that the variable is free and may differ (but it isn't required).\n",
    "\n",
    "```\n",
    "|       | Architecture | Bitness | Compiler | Version | Optimization |\n",
    "|-------|--------------|---------|----------|---------|--------------|\n",
    "| arch  | X            |         |          |         |              |\n",
    "| bit   |              | X       |          |         |              |\n",
    "| comp  |              |         | X        | X       |              |\n",
    "| ver   |              |         |          | X       |              |\n",
    "| opt   |              |         |          |         | X            |\n",
    "| XA    | X            | X       |          |         |              |\n",
    "| XA+XO | X            | X       |          |         | X            |\n",
    "| XC    |              |         | X        | X       | X            |\n",
    "| XC+XB |              | X       | X        | X       | X            |\n",
    "| XM    | *            | *       | *        | *       | *            |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2b41299",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\n",
    "    \"project\",\n",
    "    \"library\",\n",
    "    \"arch\",\n",
    "    \"bit\",\n",
    "    \"compiler\",\n",
    "    \"version\",\n",
    "    \"optimizations\",\n",
    "]\n",
    "\n",
    "TASKS_DICT = {\n",
    "    # For any positive pair, the project and the library are the same.\n",
    "    #   True: the variable is required to have the same value in the positive pair\n",
    "    #   False: the variable is required to have different values in the negative pair.\n",
    "    \"arch\": [\n",
    "        True, True, False, True, True, True, True],\n",
    "    \"bit\": [\n",
    "        True, True, True, False, True, True, True],\n",
    "    \"comp\": [\n",
    "        True, True, True, True, False, False, True],\n",
    "    \"ver\": [\n",
    "        True, True, True, True, True, False, True],\n",
    "    \"opt\": [\n",
    "        True, True, True, True, True, True, False],\n",
    "    \"XA\": [\n",
    "        True, True, False, False, True, True, True],\n",
    "    \"XA+XO\": [\n",
    "        True, True, False, False, True, True, False],\n",
    "    \"XC\": [\n",
    "        True, True, True, True, False, False, False],\n",
    "    \"XC+XB\": [\n",
    "        True, True, True, False, False, False, False],\n",
    "    # The following would be the XA+XC test\n",
    "    # \"XA+XC\": [\n",
    "    #    True, True, False, False, False, False, False]\n",
    "}\n",
    "\n",
    "# The XO test is the same as the opt one.\n",
    "TASKS_DICT[\"XO\"] = TASKS_DICT[\"opt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4639b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TWO_DICT = {\n",
    "    \"eval\": {\n",
    "        \"test\": {\n",
    "            \"similarity\": {\n",
    "                \"XA\": 50000,\n",
    "                \"XA+XO\": 50000,\n",
    "                \"XO\": 50000\n",
    "            },\n",
    "            \"rank\": {\"XA\": 200, \"XA+XO\": 200, \"XO\": 200},\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c59899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save the new dataset\n",
    "OUTPUT_DIR = \"../Dataset-2\"\n",
    "\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"[D] DIR created: {OUTPUT_DIR}\")\n",
    "\n",
    "tmp_path = os.path.join(OUTPUT_DIR, \"pairs\")\n",
    "if not os.path.isdir(tmp_path):\n",
    "    os.makedirs(tmp_path)\n",
    "    print(f\"[D] DIR created: {tmp_path}\")\n",
    "\n",
    "tmp_path = os.path.join(OUTPUT_DIR, \"features\")\n",
    "if not os.path.isdir(tmp_path):\n",
    "    os.makedirs(tmp_path)\n",
    "    print(f\"[D] DIR created: {tmp_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4492f",
   "metadata": {},
   "source": [
    "### Select functions for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6f96b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SameFileError",
     "evalue": "'features/flowchart_Dataset-2.csv' and '../Dataset-2/features/flowchart_Dataset-2.csv' are the same file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSameFileError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m CSV_FLOWCHART_FP \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures/flowchart_Dataset-2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Copy the flowchart file to the new folder\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_FLOWCHART_FP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflowchart_Dataset-2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/shutil.py:417\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    416\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 417\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/shutil.py:234\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    231\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutil.copyfile\u001b[39m\u001b[38;5;124m\"\u001b[39m, src, dst)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _samefile(src, dst):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SameFileError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m are the same file\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(src, dst))\n\u001b[1;32m    236\u001b[0m file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([src, dst]):\n",
      "\u001b[0;31mSameFileError\u001b[0m: 'features/flowchart_Dataset-2.csv' and '../Dataset-2/features/flowchart_Dataset-2.csv' are the same file"
     ]
    }
   ],
   "source": [
    "# The starting point\n",
    "CSV_FLOWCHART_FP = \"features/flowchart_Dataset-2.csv\"\n",
    "\n",
    "# Copy the flowchart file to the new folder\n",
    "shutil.copy(CSV_FLOWCHART_FP, os.path.join(OUTPUT_DIR, \"features\", \"flowchart_Dataset-2.csv\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa99c91",
   "metadata": {},
   "source": [
    "Summary:\n",
    "   * Step 0 - Read the list of functions from the output of IDA flowchart\n",
    "   * Step 1 -  Filter the functions with less than 5 BBs\n",
    "   * Step 2 - Remove duplicated hashopcodes to remove duplicated functions\n",
    "   * Step 3 - Extract compilation variables from idb_path\n",
    "   * Step 4 - Remove singleton functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1e1b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (609585, 8)\n"
     ]
    }
   ],
   "source": [
    "# Step0 - Read the list of functions from the output of IDA flowchart\n",
    "df = pd.read_csv(CSV_FLOWCHART_FP)\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85dda515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (609585, 7)\n"
     ]
    }
   ],
   "source": [
    "# Remove the column with the list of basic-blocks\n",
    "del df['bb_list']\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7110bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (609585, 7)\n"
     ]
    }
   ],
   "source": [
    "# Step1 -  Filter the functions with less than 5 BBs\n",
    "df = df[df['bb_num'] >= 5]\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef52c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (127196, 7)\n"
     ]
    }
   ],
   "source": [
    "# Step2 - Remove duplicated hashopcodes to remove duplicated functions\n",
    "df.drop_duplicates('hashopcodes', keep='first', inplace=True)\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78d5ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (127196, 14)\n"
     ]
    }
   ],
   "source": [
    "# Step3 - Extract compilation variables from idb_path\n",
    "compilation_var = list()\n",
    "for path in df['idb_path']:\n",
    "    slist = path.split(\"/\")[2].split(\"_\")\n",
    "    arch, bit = slist[0].split(\"-\")\n",
    "    splits = slist[1].split(\"-\")\n",
    "    library = \"-\".join(splits[:-1])\n",
    "    project = library.split(\"-\")[0]\n",
    "    comp = \"gcc\"\n",
    "    ver = \"7.5\"\n",
    "    opt = splits[-1]\n",
    "    compilation_var.append([project, library, arch, bit, comp, ver, opt])\n",
    "\n",
    "# Convert to NumPy Array\n",
    "compilation_var = np.array(compilation_var)\n",
    "\n",
    "# Add compilation variables to the DataFrame\n",
    "df['project'] = compilation_var[:,0].tolist()\n",
    "df['library'] = compilation_var[:,1].tolist()\n",
    "df['arch'] = compilation_var[:,2].tolist()\n",
    "df['bit'] = compilation_var[:,3].tolist()\n",
    "df['compiler'] = compilation_var[:,4].tolist()\n",
    "df['version'] = compilation_var[:,5].tolist()\n",
    "df['optimizations'] = compilation_var[:,6].tolist()\n",
    "\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef50eeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] # function to remove: 3910\n",
      "[D] Shape: (123286, 14)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step4 - Remove singleton functions\n",
    "sl = [x for x, y in df[[\"library\", \"func_name\"]].value_counts().items() if y < 2]\n",
    "gg = df.groupby([\"library\", \"func_name\"]).groups\n",
    "idx_list = list(chain(*[list(gg[i]) for i in sl]))\n",
    "print(f\"[D] # function to remove: {len(idx_list)}\")\n",
    "\n",
    "df.drop(idx_list, inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(f\"[D] Shape: {df.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3f852f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape df: \t\t(123286, 14)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape df: \\t\\t{df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96b47b",
   "metadata": {},
   "source": [
    "### Create positive and negative pairs for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ba7c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_pairs(df_input, num_pairs, test):\n",
    "    \"\"\"\n",
    "    Generate \"num_pairs\" positive function pairs by sub sampling all the\n",
    "    possible function combinations. Use this function when the number\n",
    "    of ((libraries, function_names)) is limited to few hundreds.\n",
    "    \"\"\"\n",
    "    # Map (libraries, function_names) to the indexes in the DB\n",
    "    libfunc_dict = {\n",
    "        k: list(v) for k, v in df_input.groupby([\"library\", \"func_name\"]).groups.items()\n",
    "    }\n",
    "\n",
    "    pos_pair_set = set()\n",
    "    neg_pair_set = set()\n",
    "    pos_pair_list = list()\n",
    "    neg_pair_list = list()\n",
    "\n",
    "    # Iterate over each library/func_name pair\n",
    "    for entry in tqdm(libfunc_dict.keys(), ncols=100):\n",
    "        libname, fname = entry\n",
    "\n",
    "        # Get the list of indexes associated to the ((libname, fname)) pair\n",
    "        idx_libfunc = libfunc_dict[entry]\n",
    "        # DataFrame for the library/func_name pair\n",
    "        df_libfunc = df_input.iloc[idx_libfunc]\n",
    "\n",
    "        # Get the list of indexes to select negative pairs\n",
    "        idx_list_neg = df_input[df_input[\"func_name\"] != fname].index\n",
    "\n",
    "        # (<-- left) Iterate over each function for the ((libname, fname)) pair\n",
    "        for idx_left_p in idx_libfunc:\n",
    "\n",
    "            # Extract the compilation variables\n",
    "            comp_data = df_input.iloc[idx_left_p][CATEGORIES].values\n",
    "\n",
    "            # For the XM test, any combination is valid\n",
    "            idx_list_pos = idx_libfunc\n",
    "\n",
    "            if test != \"XM\":\n",
    "                mask = TASKS_DICT[test]\n",
    "                # Build the constraints dict\n",
    "                #   if m is True: the variable is required to be the same in the positive pair\n",
    "                fd = {c: v for m, c, v in zip(mask, CATEGORIES, comp_data) if m}\n",
    "                constraints = [(df_libfunc[k] == v) for k, v in fd.items()]\n",
    "                #   if m is False: the variable is required to be different in the positive pair.\n",
    "                fd = {c: v for m, c, v in zip(mask, CATEGORIES, comp_data) if not m}\n",
    "                constraints += [(df_libfunc[k] != v) for k, v in fd.items()]\n",
    "\n",
    "                # Get the list of indexes of candidate right functions to generate positive pairs\n",
    "                idx_list_pos = df_libfunc[np.logical_and.reduce(constraints)].index\n",
    "\n",
    "            # Remove the left function from the list\n",
    "            idx_list_pos = [idx for idx in idx_list_pos if idx != idx_left_p]\n",
    "\n",
    "            # Iterate over each (--> right) function\n",
    "            for idx_right_p in idx_list_pos:\n",
    "                pos_pair = (idx_left_p, idx_right_p)\n",
    "\n",
    "                # Check if the pos_pair is already in the list\n",
    "                if tuple(sorted(pos_pair)) not in pos_pair_set:\n",
    "                    pos_pair_set.add(tuple(sorted(pos_pair)))\n",
    "                    pos_pair_list.append(pos_pair)\n",
    "\n",
    "                    # Generate the corresponding negative pair\n",
    "                    is_success = False\n",
    "                    while not is_success:\n",
    "                        idx_right_n = random.choice(idx_list_neg)\n",
    "                        neg_pair = (idx_left_p, idx_right_n)\n",
    "\n",
    "                        # Check if the neg_pair is already in the list\n",
    "                        if tuple(sorted(neg_pair)) not in neg_pair_set:\n",
    "                            neg_pair_set.add(tuple(sorted(neg_pair)))\n",
    "                            neg_pair_list.append(neg_pair)\n",
    "                            is_success = True\n",
    "\n",
    "    # print(\n",
    "    #     f\"[D] Before sampling - pos: {len(pos_pair_list)} - neg: {len(neg_pair_list)}\"\n",
    "    # )\n",
    "\n",
    "    # Sub sample the positive and negative pairs to num_pairs\n",
    "    if len(pos_pair_list) > num_pairs:\n",
    "        sampled_list = random.sample(list(range(len(pos_pair_list))), num_pairs)\n",
    "        pos_pair_list = [pos_pair_list[x] for x in sampled_list]\n",
    "        neg_pair_list = [neg_pair_list[x] for x in sampled_list]\n",
    "        # print(\n",
    "        #     f\"[D] After sampling - pos: {len(pos_pair_list)} - neg: {len(neg_pair_list)}\"\n",
    "        # )\n",
    "\n",
    "    return pos_pair_list, neg_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "002e39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_pairs_random_version(df_input, num_pairs, test, num_negatives=1):\n",
    "    \"\"\"\n",
    "    Randomly generate \"num_pairs\" positive function pairs. Use this function\n",
    "    when the number of (libraries, function_names) pairs is > 1 thousand.\n",
    "    \"\"\"\n",
    "    # Map (libraries, function_names) to the indexes in the DB\n",
    "    libfunc_dict = {\n",
    "        k: list(v) for k, v in df_input.groupby([\"library\", \"func_name\"]).groups.items()\n",
    "    }\n",
    "    libfunc_list = list(libfunc_dict.keys())\n",
    "\n",
    "    pos_pair_set = set()\n",
    "    neg_pair_set = set()\n",
    "    pos_pair_list = list()\n",
    "    neg_pair_list = list()\n",
    "\n",
    "    with tqdm(total=num_pairs, ncols=100) as pbar:\n",
    "        # Iterate num_pairs time to create the pos/neg function pairs\n",
    "        for _ in range(num_pairs):\n",
    "\n",
    "            # Iterate until a positive function pair is generated\n",
    "            is_success_pos = False\n",
    "            while not is_success_pos:\n",
    "\n",
    "                # Randomly select a library/func_name pair\n",
    "                entry = random.choice(libfunc_list)\n",
    "                libname, fname = entry\n",
    "                # Get the list of indexes associated to the library/func_name pair\n",
    "                idx_libfunc = libfunc_dict[entry]\n",
    "                # DataFrame for the library/func_name pair\n",
    "                df_libfunc = df_input.iloc[idx_libfunc]\n",
    "\n",
    "                # Randomly select a (<-- left) function\n",
    "                idx_left_p = random.choice(idx_libfunc)\n",
    "                # Extract the compilation variables\n",
    "                comp_data = df_input.iloc[idx_left_p][CATEGORIES].values\n",
    "\n",
    "                # For the XM test, any combination is valid\n",
    "                idx_list_pos = idx_libfunc\n",
    "\n",
    "                if test != \"XM\":\n",
    "                    mask = TASKS_DICT[test]\n",
    "                    # Build the constraints dict\n",
    "                    #   if m is True: the variable is required to be the same in the positive pair\n",
    "                    fd = {c: v for m, c, v in zip(mask, CATEGORIES, comp_data) if m}\n",
    "                    constraints = [(df_libfunc[k] == v) for k, v in fd.items()]\n",
    "                    #   if m is False: the variable is required to be different in the positive pair.\n",
    "                    fd = {c: v for m, c, v in zip(mask, CATEGORIES, comp_data) if not m}\n",
    "                    constraints += [(df_libfunc[k] != v) for k, v in fd.items()]\n",
    "\n",
    "                    # Get the list of indexes of candidate right functions to generate positive pairs\n",
    "                    idx_list_pos = df_libfunc[np.logical_and.reduce(constraints)].index\n",
    "\n",
    "                # Remove the left function from the list\n",
    "                idx_list_pos = [idx for idx in idx_list_pos if idx != idx_left_p]\n",
    "\n",
    "                # No functions are left. Retry\n",
    "                if len(idx_list_pos) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Randomly select a (<-- right) function\n",
    "                idx_right_p = random.choice(idx_list_pos)\n",
    "                pos_pair = (idx_left_p, idx_right_p)\n",
    "                if tuple(sorted(pos_pair)) not in pos_pair_set:\n",
    "                    pos_pair_set.add(tuple(sorted(pos_pair)))\n",
    "                    pos_pair_list.append(pos_pair)\n",
    "                    is_success_pos = True\n",
    "\n",
    "                    for _ in range(num_negatives):\n",
    "                        # Generate the corresponding negative pair\n",
    "                        is_success_neg = False\n",
    "                        while not is_success_neg:\n",
    "                            idx_right_n = random.randint(0, df_input.shape[0] - 1)\n",
    "                            if df_input.iloc[idx_right_n][\"func_name\"] == fname:\n",
    "                                continue\n",
    "                            neg_pair = (idx_left_p, idx_right_n)\n",
    "\n",
    "                            # Check if the neg_pair is already in the list\n",
    "                            if tuple(sorted(neg_pair)) not in neg_pair_set:\n",
    "                                neg_pair_set.add(tuple(sorted(neg_pair)))\n",
    "                                neg_pair_list.append(neg_pair)\n",
    "                                is_success_neg = True\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "        # print(f\"[D] pos: {len(pos_pair_list)} - neg: {len(neg_pair_list)}\")\n",
    "\n",
    "    return pos_pair_list, neg_pair_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2adc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dicts_into_dataframes(df_input, dataset_dict):\n",
    "    pair_columns = [\n",
    "        \"idb_path_1\",\n",
    "        \"fva_1\",\n",
    "        \"func_name_1\",\n",
    "        \"idb_path_2\",\n",
    "        \"fva_2\",\n",
    "        \"func_name_2\",\n",
    "        \"db_type\",\n",
    "    ]\n",
    "\n",
    "    pos_pair_dict = defaultdict(list)\n",
    "    neg_pair_dict = defaultdict(list)\n",
    "    \n",
    "    # Iterate over each positive and negative pair.\n",
    "    #   Select the required info and save it in a new dictionary.\n",
    "    for task in dataset_dict:\n",
    "        for pos_pair in dataset_dict[task][\"pos\"]:\n",
    "            for c in [\"idb_path\", \"fva\", \"func_name\"]:\n",
    "                pos_pair_dict[c + \"_1\"].append(df_input.iloc[pos_pair[0]][c])\n",
    "                pos_pair_dict[c + \"_2\"].append(df_input.iloc[pos_pair[1]][c])\n",
    "            pos_pair_dict[\"db_type\"].append(task)\n",
    "\n",
    "        for neg_pair in dataset_dict[task][\"neg\"]:\n",
    "            for c in [\"idb_path\", \"fva\", \"func_name\"]:\n",
    "                neg_pair_dict[c + \"_1\"].append(df_input.iloc[neg_pair[0]][c])\n",
    "                neg_pair_dict[c + \"_2\"].append(df_input.iloc[neg_pair[1]][c])\n",
    "            neg_pair_dict[\"db_type\"].append(task)\n",
    "    \n",
    "    # Convert the local pair_dicts into DataFrames\n",
    "    df_pos = pd.DataFrame.from_dict(pos_pair_dict)\n",
    "    df_neg = pd.DataFrame.from_dict(neg_pair_dict)\n",
    "    \n",
    "    # Check/change the order of the columns\n",
    "    df_pos = df_pos[pair_columns]\n",
    "    df_neg = df_neg[pair_columns]\n",
    "    return df_pos, df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1498aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(dataset_dict):\n",
    "    print(\"[D] Summary:\") \n",
    "    for task in dataset_dict:\n",
    "        print(\n",
    "            \"[D] \\tTask: {:5} - pos: {:5} neg: {:5}\".format(\n",
    "                task, len(dataset_dict[task][\"pos\"]), len(dataset_dict[task][\"neg\"])\n",
    "            )\n",
    "        )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60a6c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_free_variables(df_input, task_list, dataset_dict):\n",
    "    for task in task_list:\n",
    "        # Skip \"XM\"\n",
    "        if task not in TASKS_DICT:\n",
    "            continue\n",
    "\n",
    "        print(\"-\" * 100 + \"\\n\")\n",
    "        print(f\"[D] Task: {task}\\n\")\n",
    "\n",
    "        # Get the name of the free variables for each task\n",
    "        free_variables = list(\n",
    "            compress(CATEGORIES, [not x for x in TASKS_DICT[task]])\n",
    "        )\n",
    "\n",
    "        v_list = list()\n",
    "        for pos_pair in dataset_dict[task][\"pos\"]:\n",
    "            # Get the values associated to the free variables\n",
    "            vv = df_input.iloc[list(pos_pair)][free_variables].values\n",
    "            # Sort them to avoid counting the permutations\n",
    "            vv = tuple(sorted([tuple(x) for x in vv]))\n",
    "            v_list.append(vv)\n",
    "\n",
    "        # Print the frequency of each combination\n",
    "        for k, v in Counter(v_list).most_common():\n",
    "            print(f\"\\t{v:5}, {k}\")\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f9ab051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_neg_dataset(\n",
    "    df_input, task_dict, output_dir, output_fs, rand=True, num_negatives=1\n",
    "):\n",
    "    print(\"[D] Creating the pos/neg function pairs...\", flush=True)\n",
    "    dataset_dict = defaultdict(dict)\n",
    "\n",
    "    for task, num_pairs in task_dict.items():\n",
    "        ppl, npl = None, None\n",
    "        if rand:\n",
    "            # Use the random version of the pair generation function\n",
    "            ppl, npl = create_similarity_pairs_random_version(\n",
    "                df_input, num_pairs, task, num_negatives\n",
    "            )\n",
    "        else:\n",
    "            ppl, npl = create_similarity_pairs(df_input, num_pairs, task)\n",
    "        dataset_dict[task][\"pos\"] = ppl\n",
    "        dataset_dict[task][\"neg\"] = npl\n",
    "\n",
    "    print_summary(dataset_dict)\n",
    "\n",
    "    print(\"[D] Converting the positive/negative pairs into CSV...\", flush=True)\n",
    "    df_pos, df_neg = convert_dicts_into_dataframes(df_input, dataset_dict)\n",
    "\n",
    "    pos_fp = os.path.join(output_dir, \"pos\" + output_fs)\n",
    "    df_pos.to_csv(pos_fp)\n",
    "    print(f\"[D] \\tPos CSV: {pos_fp}\")\n",
    "\n",
    "    neg_fp = os.path.join(output_dir, \"neg\" + output_fs)\n",
    "    df_neg.to_csv(neg_fp)\n",
    "    print(f\"[D] \\tNeg CSV: {neg_fp}\")\n",
    "\n",
    "    # For debug only\n",
    "    print_free_variables(df_input, task_dict.keys(), dataset_dict)\n",
    "    \n",
    "    selected_functions = set()\n",
    "    for task in dataset_dict:\n",
    "        for pair in dataset_dict[task][\"pos\"]:\n",
    "            selected_functions.update(list(pair))\n",
    "        for pair in dataset_dict[task][\"neg\"]:\n",
    "            selected_functions.update(list(pair))\n",
    "    return selected_functions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05ea13cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MultiProcessCollector:\n",
    "    def __init__(self, threshold: int) -> None:\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.pos_pair_set = set()\n",
    "        self.neg_pair_set = set()\n",
    "        self.pos_pair_list = list()\n",
    "        self.neg_pair_list = list()\n",
    "        \n",
    "    def start_collector(self, input_channel: multiprocessing.Queue, output_channel: multiprocessing.Queue):\n",
    "        bar = tqdm(total=self.threshold, desc=\"Collected Pairs\")\n",
    "        while True:\n",
    "            \n",
    "            if not output_channel.empty():\n",
    "                signal = output_channel.get()\n",
    "                if isinstance(signal, str) and signal == \"STOP\":\n",
    "                    break\n",
    "            \n",
    "            pos_pair_list_recv, neg_pair_list_recv = input_channel.get()\n",
    "            if not len(pos_pair_list_recv) == len(neg_pair_list_recv):\n",
    "                print(f\"pos_pair_list_recv: {len(pos_pair_list_recv)}, neg_pair_list_recv: {len(neg_pair_list_recv)}\")\n",
    "                print(f\"pos_pair_list_recv: {pos_pair_list_recv}\")\n",
    "                print(f\"neg_pair_list_recv: {neg_pair_list_recv}\")\n",
    "                raise ValueError(\"The length of the positive and negative pairs are not the same\")\n",
    "            \n",
    "            \n",
    "            for i in range(len(pos_pair_list_recv)):\n",
    "                pos_pair = pos_pair_list_recv[i]\n",
    "                neg_pair = neg_pair_list_recv[i]\n",
    "                if tuple(sorted(pos_pair)) not in self.pos_pair_set and tuple(sorted(neg_pair)) not in self.neg_pair_set:\n",
    "                    self.pos_pair_set.add(tuple(sorted(pos_pair)))\n",
    "                    self.neg_pair_set.add(tuple(sorted(neg_pair)))\n",
    "                    self.pos_pair_list.append(pos_pair)\n",
    "                    self.neg_pair_list.append(neg_pair)\n",
    "                    bar.update(1)\n",
    "                    \n",
    "            if len(self.pos_pair_list) >= self.threshold:\n",
    "                output_channel.put(\"STOP\")\n",
    "                break\n",
    "        \n",
    "        output_channel.put((self.pos_pair_list, self.neg_pair_list))\n",
    "        bar.close()\n",
    "\n",
    "class MultiProcessWorker:\n",
    "    def __init__(self, df_input: pd.DataFrame, test: str, libfunc_dict: dict, categories: list, task_dict: dict):\n",
    "        self.df_input = df_input\n",
    "        self.test = test\n",
    "        self.libfunc_dict = libfunc_dict\n",
    "        self.categories = categories\n",
    "        self.task_dict = task_dict\n",
    "\n",
    "    def start_worker(self, input_queue: multiprocessing.Queue, output_queue: multiprocessing.Queue):\n",
    "        while True:\n",
    "            entry = input_queue.get()\n",
    "            if isinstance(entry, str) and entry == \"STOP\":\n",
    "                break\n",
    "            \n",
    "            res = self.handle_entry(entry)\n",
    "            output_queue.put(res)\n",
    "\n",
    "    def handle_entry(self, entry):\n",
    "        idx_libfunc = self.libfunc_dict[entry]\n",
    "        df_libfunc = self.df_input.iloc[idx_libfunc]\n",
    "        \n",
    "        pos_pair_list = []\n",
    "        neg_pair_list = []\n",
    "        \n",
    "        for idx_left_p in idx_libfunc:\n",
    "            comp_data = self.df_input.iloc[idx_left_p][self.categories].values\n",
    "            idx_list_pos = idx_libfunc\n",
    "            if self.test != \"XM\":\n",
    "                # might use the query to speed up the process\n",
    "                df_libfunc: pd.DataFrame = self.df_input.iloc[idx_libfunc]\n",
    "                mask = self.task_dict[self.test]\n",
    "\n",
    "                condition = ''\n",
    "\n",
    "                for m, c, v in zip(mask, self.categories, comp_data):\n",
    "                    if isinstance(v, str):\n",
    "                        condition += f'({c} == \"{v}\") and ' if m else f'({c} != \"{v}\") and '\n",
    "                    else:\n",
    "                        condition += f'({c} == {v}) and ' if m else f'({c} != {v}) and '\n",
    "\n",
    "                idx_list_pos = df_libfunc.query(condition[:-5]).index\n",
    "\n",
    "            idx_list_pos = [idx for idx in idx_list_pos if idx != idx_left_p]\n",
    "            for idx_right_p in idx_list_pos:\n",
    "                pos_pair = (idx_left_p, idx_right_p)\n",
    "                pos_pair_list.append(pos_pair)\n",
    "                is_success = False\n",
    "                while not is_success:\n",
    "                    idx_right_n = random.choice(range(len(self.df_input)))\n",
    "\n",
    "                    if self.df_input.iloc[idx_right_n][\"func_name\"] == self.df_input.iloc[idx_left_p][\"func_name\"]:\n",
    "                        continue\n",
    "\n",
    "                    neg_pair = (idx_left_p, idx_right_n)\n",
    "                    if neg_pair not in neg_pair_list:\n",
    "                        neg_pair_list.append(neg_pair)\n",
    "                        is_success = True\n",
    "\n",
    "        assert len(pos_pair_list) == len(neg_pair_list)\n",
    "        return pos_pair_list, neg_pair_list\n",
    "\n",
    "\n",
    "def spawn_collector_process_for_no_rand(threshold: int, input_queue: multiprocessing.Queue, output_queue: multiprocessing.Queue, lock: multiprocessing.Lock):\n",
    "    collector_process = MultiProcessCollector(threshold)\n",
    "    lock.acquire()\n",
    "    collector_process.start_collector(input_queue, output_queue)\n",
    "    lock.release()\n",
    "    \n",
    "def spawn_worker_process_for_no_rand(df_input: pd.DataFrame, test: str, libfunc_dict: dict, categories: list, task_dict: dict, input_queue: multiprocessing.Queue, output_queue: multiprocessing.Queue, lock: multiprocessing.Lock):\n",
    "    worker_process = MultiProcessWorker(df_input, test, libfunc_dict, categories, task_dict)\n",
    "    lock.acquire()\n",
    "    worker_process.start_worker(input_queue, output_queue)\n",
    "    lock.release()\n",
    "    \n",
    "\n",
    "def create_similarity_pairs_parallel(df_input: pd.DataFrame, num_pairs: int, test: str):\n",
    "    libfunc_dict = {\n",
    "        k: list(v) for k, v in df_input.groupby([\"library\", \"func_name\"]).groups.items()\n",
    "    }\n",
    "\n",
    "    threshold = min(len(libfunc_dict), num_pairs)\n",
    "\n",
    "    manager = multiprocessing.Manager()\n",
    "    \n",
    "    collector_lock = multiprocessing.Lock()\n",
    "    collector_lock.acquire()\n",
    "\n",
    "    cpu_count = multiprocessing.cpu_count()\n",
    "    \n",
    "    worker_input_queue = manager.Queue(maxsize=cpu_count*10)\n",
    "    worker_output_queue = manager.Queue(maxsize=cpu_count*10)\n",
    "    collector_output_queue = manager.Queue(maxsize=cpu_count*10)\n",
    "    \n",
    "    \n",
    "    collector_process = multiprocessing.Process(target=spawn_collector_process_for_no_rand, args=(threshold, worker_output_queue, collector_output_queue, collector_lock))\n",
    "    \n",
    "    worker_locks = [multiprocessing.Lock() for _ in range(cpu_count-1)]\n",
    "    worker_processes = []\n",
    "    \n",
    "    for i in range(cpu_count-1):\n",
    "        worker_locks[i].acquire()\n",
    "        worker_process = multiprocessing.Process(target=spawn_worker_process_for_no_rand, args=(df_input, test, libfunc_dict, CATEGORIES, TASKS_DICT, worker_input_queue, worker_output_queue, worker_locks[i]))\n",
    "        worker_processes.append(worker_process)\n",
    "        \n",
    "    collector_process.start()\n",
    "    collector_lock.release()\n",
    "    \n",
    "    for i in range(len(worker_processes)):\n",
    "        worker_processes[i].start()\n",
    "        worker_locks[i].release()\n",
    "        \n",
    "    # Start give task to the worker processes\n",
    "    bar_main = tqdm(total=len(libfunc_dict), desc=\"All Function consumed\")\n",
    "    for entry in libfunc_dict.keys():\n",
    "        worker_input_queue.put(entry)\n",
    "        bar_main.update(1)\n",
    "        if not collector_output_queue.empty():\n",
    "            signal = collector_output_queue.get()\n",
    "            if isinstance(signal, str) and signal == \"STOP\":\n",
    "                break\n",
    "    bar_main.close()\n",
    "            \n",
    "    collector_output_queue.put(\"STOP\")\n",
    "    \n",
    "    for i in range(len(worker_processes)):\n",
    "        worker_input_queue.put(\"STOP\")\n",
    "        \n",
    "    \n",
    "    while not worker_output_queue.empty():\n",
    "        _ = worker_output_queue.get()\n",
    "    \n",
    "    for i in range(len(worker_processes)):\n",
    "        worker_processes[i].join()\n",
    "    \n",
    "    collector_process.join()\n",
    "    \n",
    "    while True:\n",
    "        result = collector_output_queue.get()\n",
    "        if isinstance(result, str) and result == \"STOP\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    pos_pair_list, neg_pair_list = result\n",
    "    \n",
    "    print(\n",
    "        f\"[D] Before sampling - pos: {len(pos_pair_list)} - neg: {len(neg_pair_list)}\"\n",
    "    )\n",
    "\n",
    "    # Sub sample the positive and negative pairs to num_pairs\n",
    "    if len(pos_pair_list) > num_pairs:\n",
    "        sampled_list = random.sample(list(range(len(pos_pair_list))), num_pairs)\n",
    "        pos_pair_list = [pos_pair_list[x] for x in sampled_list]\n",
    "        neg_pair_list = [neg_pair_list[x] for x in sampled_list]\n",
    "        print(\n",
    "            f\"[D] After sampling - pos: {len(pos_pair_list)} - neg: {len(neg_pair_list)}\"\n",
    "        )\n",
    "\n",
    "    return pos_pair_list, neg_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f36e2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import multiprocessing\n",
    "\n",
    "class RandomPairsWorker:\n",
    "    def __init__(self, df_input: pd.DataFrame, libfunc_dict: dict, test: str, num_negatives: int, task_dict: dict, categories: list, scaler: float = 1.5):\n",
    "        self.df_input = df_input\n",
    "        self.libfunc_dict = libfunc_dict\n",
    "        self.test = test\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "        self.categories = categories\n",
    "        self.task_dict = task_dict\n",
    "        \n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def start_worker(self, input_queue: multiprocessing.Queue, output_queue: multiprocessing.Queue):\n",
    "        while True:\n",
    "            entry = input_queue.get()\n",
    "            if isinstance(entry, str) and entry == \"STOP\":\n",
    "                break\n",
    "            succ, pos_pair_list, neg_pair_list = self.handle_one(entry)\n",
    "            if succ:\n",
    "                output_queue.put((pos_pair_list, neg_pair_list))\n",
    "        \n",
    "    def handle_one(self, entry):\n",
    "        \n",
    "        pos_pair_list = []\n",
    "        \n",
    "        total_pos_pair_list = []\n",
    "        total_neg_pair_list = []\n",
    "        \n",
    "        is_success_pos = False\n",
    "\n",
    "        # Randomly select a library/func_name pair\n",
    "        libname, fname = entry\n",
    "        # Get the list of indexes associated to the library/func_name pair\n",
    "        idx_libfunc = self.libfunc_dict[entry]\n",
    "        # DataFrame for the library/func_name pair\n",
    "        df_libfunc = self.df_input.iloc[idx_libfunc]\n",
    "\n",
    "        # Randomly select a (<-- left) function\n",
    "        idx_left_p = random.choice(idx_libfunc)\n",
    "        \n",
    "        for idx_left_p in idx_libfunc:\n",
    "            # Extract the compilation variables\n",
    "            comp_data = self.df_input.iloc[idx_left_p][self.categories].values\n",
    "\n",
    "            # For the XM test, any combination is valid\n",
    "            idx_list_pos = idx_libfunc\n",
    "\n",
    "            if self.test != \"XM\":\n",
    "                mask = self.task_dict[self.test]\n",
    "                # Build the constraints dict\n",
    "                #   if m is True: the variable is required to be the same in the positive pair\n",
    "                fd = {c: v for m, c, v in zip(mask, self.categories, comp_data) if m}\n",
    "                constraints = [(df_libfunc[k] == v) for k, v in fd.items()]\n",
    "                #   if m is False: the variable is required to be different in the positive pair.\n",
    "                fd = {c: v for m, c, v in zip(mask, self.categories, comp_data) if not m}\n",
    "                constraints += [(df_libfunc[k] != v) for k, v in fd.items()]\n",
    "\n",
    "                # Get the list of indexes of candidate right functions to generate positive pairs\n",
    "                idx_list_pos = df_libfunc[np.logical_and.reduce(constraints)].index\n",
    "                \n",
    "                # need to use query\n",
    "                # query_str = \"\"\n",
    "                \n",
    "                # for m, c, v in zip(mask, self.categories, comp_data):\n",
    "                #     if m:\n",
    "                #         if isinstance(v, str):\n",
    "                #             query_str += f\"{c} == '{v}' and \"\n",
    "                #         else:\n",
    "                #             query_str += f\"{c} == {v} and \"\n",
    "                #     else:\n",
    "                #         if isinstance(v, str):\n",
    "                #             query_str += f\"{c} != '{v}' and \"\n",
    "                #         else:\n",
    "                #             query_str += f\"{c} != {v} and \"\n",
    "                            \n",
    "                # idx_list_pos = df_libfunc.query(query_str[:-5]).index\n",
    "\n",
    "            # Remove the left function from the list\n",
    "            idx_list_pos = [idx for idx in idx_list_pos if idx != idx_left_p]\n",
    "\n",
    "            number_of_positives = len(idx_list_pos)\n",
    "            number_of_positives = random.randint(0, number_of_positives)\n",
    "            idx_list_pos = random.sample(idx_list_pos, number_of_positives)\n",
    "\n",
    "            # No functions are left. Retry\n",
    "            if len(idx_list_pos) == 0:\n",
    "                continue\n",
    "\n",
    "            # Randomly select a (<-- right) function\n",
    "            \n",
    "            for idx_right_p in idx_list_pos:\n",
    "                \n",
    "                pos_pair = (idx_left_p, idx_right_p)\n",
    "\n",
    "                pos_pair_list.append(pos_pair)\n",
    "                \n",
    "                # At lease one pair is successfully generated\n",
    "                is_success_pos = True\n",
    "\n",
    "                is_success_neg = False\n",
    "                neg_pair_list = []\n",
    "                neg_pair_set = set()\n",
    "                \n",
    "                for _ in range(round(self.num_negatives * self.scaler)):\n",
    "                    # Generate the corresponding negative pair\n",
    "                    is_success_neg = False\n",
    "                    while not is_success_neg:\n",
    "                        idx_right_n = random.randint(0, self.df_input.shape[0] - 1)\n",
    "                        if self.df_input.iloc[idx_right_n][\"func_name\"] == fname:\n",
    "                            continue\n",
    "                        neg_pair = (idx_left_p, idx_right_n)\n",
    "\n",
    "                        # Check if the neg_pair is already in the list\n",
    "                        if tuple(sorted(neg_pair)) not in neg_pair_set:\n",
    "                            neg_pair_set.add(tuple(sorted(neg_pair)))\n",
    "                            neg_pair_list.append(neg_pair)\n",
    "                            is_success_neg = True\n",
    "                            \n",
    "                if is_success_neg:\n",
    "                    total_pos_pair_list.append(pos_pair)\n",
    "                    total_neg_pair_list.append(neg_pair_list)\n",
    "                    pos_pair_list = []\n",
    "                    neg_pair_list = []\n",
    "                    neg_pair_set = set()\n",
    "                    \n",
    "        if is_success_pos and is_success_neg:\n",
    "            return True, total_pos_pair_list, total_neg_pair_list\n",
    "        else:\n",
    "            return False, [], []\n",
    "\n",
    "\n",
    "class RandomPairsCollector:\n",
    "    def __init__(self, threshold: int, num_negatives: int, test: str, scaler: float = 1.5):\n",
    "        self.threshold = threshold\n",
    "        self.test = test\n",
    "        self.num_negatives = num_negatives\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def start_collector(self, input_queue: multiprocessing.Queue, output_queue: multiprocessing.Queue):\n",
    "        \n",
    "\n",
    "        bar = tqdm(total=round(self.threshold * self.scaler), desc=\"Collecting Pairs\", position=1)\n",
    "\n",
    "        \n",
    "        pos_pair_list = []\n",
    "        neg_pair_list = []\n",
    "        pos_pair_set = set()\n",
    "        neg_pair_set = set()\n",
    "        \n",
    "        attempt = 0\n",
    "        \n",
    "        while True:\n",
    "            if not output_queue.empty():\n",
    "                msg = output_queue.get()\n",
    "                if isinstance(msg, str) and msg == \"STOP\":\n",
    "                    break\n",
    "            \n",
    "            if input_queue.empty():\n",
    "                continue\n",
    "            \n",
    "            total_pos_pair_list_recv, total_neg_pair_list_recv = input_queue.get()\n",
    "            \n",
    "            if len(total_pos_pair_list_recv) != len(total_neg_pair_list_recv):\n",
    "                print(f\"[E] Positive Pair Recv Length: {len(total_pos_pair_list_recv)}, Negative Pair Recv Length: {len(total_neg_pair_list_recv)}\")\n",
    "                raise ValueError(\"The number of negative pairs is not correct.\")\n",
    "            \n",
    "            attempt += len(total_pos_pair_list_recv)\n",
    "            \n",
    "            for i in range(len(total_pos_pair_list_recv)):\n",
    "            \n",
    "                pos_pair_list_recv = total_pos_pair_list_recv[i]\n",
    "                neg_pair_list_recv = total_neg_pair_list_recv[i]\n",
    "            \n",
    "                if round(self.scaler * self.num_negatives) != len(neg_pair_list_recv):\n",
    "                    print(f\"[E] Positive Pair Recv Length: {len(pos_pair_list_recv)} - {pos_pair_list_recv}, Negative Pair Recv Length: {len(neg_pair_list_recv)}\")\n",
    "                    raise ValueError(\"The number of negative pairs is not correct.\")\n",
    "            \n",
    "                \n",
    "                if tuple(sorted(list(pos_pair_list_recv))) in pos_pair_set:\n",
    "                    continue\n",
    "                \n",
    "                temp_neg_pair = []\n",
    "                for neg_pair in neg_pair_list_recv:\n",
    "                    if tuple(sorted(list(neg_pair))) in neg_pair_set:\n",
    "                        continue\n",
    "                    temp_neg_pair.append(neg_pair)\n",
    "                \n",
    "                if len(temp_neg_pair) < self.num_negatives:\n",
    "                    continue\n",
    "            \n",
    "                temp_neg_pair = temp_neg_pair[:self.num_negatives]\n",
    "            \n",
    "                pos_pair_set.add(tuple(sorted(list(pos_pair_list_recv))))\n",
    "                pos_pair_list.append(pos_pair_list_recv)\n",
    "            \n",
    "                for i in range(len(temp_neg_pair)):\n",
    "                    neg_pair_set.add(tuple(sorted(list(temp_neg_pair[i]))))\n",
    "                    neg_pair_list.append(temp_neg_pair[i])\n",
    "                bar.update(1)\n",
    "                \n",
    "            # Need to run all the functions and use random sample to pick the pairs                     \n",
    "            if len(pos_pair_list) >= round(self.threshold * self.scaler):\n",
    "                # print(\"[Collector] Threshold Reached\")\n",
    "                output_queue.put(\"STOP\")\n",
    "                while not input_queue.empty():\n",
    "                    input_queue.get()\n",
    "                break\n",
    "        \n",
    "        # print(f\"[Collector] Attempt: {attempt}, Putting Result to Main\")\n",
    "        output_queue.put((pos_pair_list, neg_pair_list))\n",
    "        bar.close()\n",
    "        \n",
    "\n",
    "def spawn_collector_process_for_rand(threshold: int, num_negatives: int, input_channel: multiprocessing.Queue, output_channel: multiprocessing.Queue, lock: multiprocessing.Lock, test: str, scaler: float = 1.5):\n",
    "    collector = RandomPairsCollector(threshold, num_negatives, test, scaler)\n",
    "    lock.acquire()\n",
    "    collector.start_collector(input_channel, output_channel)\n",
    "    lock.release()\n",
    "    \n",
    "    \n",
    "def spawn_worker_process_for_rand(df_input: pd.DataFrame, libfunc_dict: dict, test: str, num_negatives: int, task_dict: dict, categories: list, input_channel: multiprocessing.Queue, output_channel: multiprocessing.Queue, lock: multiprocessing.Lock, scaler: float = 1.5):\n",
    "    worker = RandomPairsWorker(df_input, libfunc_dict, test, num_negatives, task_dict, categories, scaler)\n",
    "    lock.acquire()\n",
    "    worker.start_worker(input_channel, output_channel)\n",
    "    lock.release()\n",
    "\n",
    "\n",
    "def create_similarity_pairs_random_version_parallel(df_input, num_pairs, test, num_negatives=1):\n",
    "    \"\"\"\n",
    "    Randomly generate \"num_pairs\" positive function pairs. Use this function\n",
    "    when the number of (libraries, function_names) pairs is > 1 thousand.\n",
    "    \"\"\"\n",
    "    libfunc_dict = {\n",
    "        k: list(v) for k, v in df_input.groupby([\"library\", \"func_name\"]).groups.items()\n",
    "    }\n",
    "    libfunc_list = list(libfunc_dict.keys())\n",
    "    \n",
    "    # random.shuffle(libfunc_list)\n",
    "    \n",
    "    manager = multiprocessing.Manager()\n",
    "    \n",
    "    collector_lock = multiprocessing.Lock()\n",
    "    collector_lock.acquire()\n",
    "    \n",
    "    cpu_count = multiprocessing.cpu_count()\n",
    "    \n",
    "    worker_input_channel = manager.Queue(maxsize=cpu_count*2)\n",
    "    worker_output_channel = manager.Queue(maxsize=cpu_count*2)\n",
    "    collector_output_channel = manager.Queue(maxsize=cpu_count*2)\n",
    "    \n",
    "    collector_process = multiprocessing.Process(target=spawn_collector_process_for_rand, args=(num_pairs, num_negatives, worker_output_channel, collector_output_channel, collector_lock, test))\n",
    "    collector_process.daemon = True\n",
    "    \n",
    "    worker_locks = [multiprocessing.Lock() for _ in range(cpu_count - 1)]\n",
    "    worker_processes = []\n",
    "    \n",
    "    for i in range(len(worker_locks)):\n",
    "        worker_locks[i].acquire()\n",
    "        worker_process = multiprocessing.Process(target=spawn_worker_process_for_rand, args=(df_input, libfunc_dict, test, num_negatives, TASKS_DICT, CATEGORIES, worker_input_channel, worker_output_channel, worker_locks[i]))\n",
    "        worker_process.daemon = True\n",
    "        worker_processes.append(worker_process)\n",
    "        \n",
    "    collector_process.start()\n",
    "    collector_lock.release()\n",
    "    \n",
    "    for i in range(len(worker_processes)):\n",
    "        worker_processes[i].start()\n",
    "        worker_locks[i].release()\n",
    "        \n",
    "\n",
    "    random.shuffle(libfunc_list)\n",
    "        \n",
    "    bar_main = tqdm(total=len(libfunc_list), desc=\"All Function Consumed\")\n",
    "    for i in range(len(libfunc_list)):\n",
    "        worker_input_channel.put(libfunc_list[i])\n",
    "        bar_main.update(1)\n",
    "        \n",
    "        if not collector_output_channel.empty():\n",
    "            msg = collector_output_channel.get()\n",
    "            if isinstance(msg, str) and msg == \"STOP\":\n",
    "                # print(\"[Main] Get STOP from Collector Process.\")\n",
    "                break\n",
    "            \n",
    "    bar_main.close()\n",
    "    \n",
    "    # print(\"[Main] Putting STOP to the worker processes and waiting for them to finish.\")\n",
    "    for i in range(len(worker_processes)):\n",
    "        worker_input_channel.put(\"STOP\")\n",
    "        \n",
    "    # Empty the worker output channel\n",
    "\n",
    "    # print(\"[Main] Emptying the worker output channel.\")\n",
    "    while not worker_output_channel.empty():\n",
    "        worker_output_channel.get()\n",
    "    \n",
    "    # print(\"[Main] Waiting for the worker processes to finish.\")\n",
    "    for i in range(len(worker_processes)):\n",
    "        while not worker_output_channel.empty():\n",
    "            worker_output_channel.get()\n",
    "        worker_processes[i].terminate()\n",
    "        worker_processes[i].join()\n",
    "    \n",
    "    # print(\"[Main] Putting STOP to the collector process and waiting for it to finish.\")\n",
    "    collector_output_channel.put(\"STOP\")\n",
    "    collector_process.join()\n",
    "\n",
    "    # print(\"[Main] Collecting Result From Collector Process.\")\n",
    "    while True:\n",
    "        result = collector_output_channel.get()\n",
    "        if isinstance(result, str) and result == \"STOP\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "\n",
    "    pos_pair_list, neg_pair_list = result\n",
    "    \n",
    "    print(\n",
    "        f\"[D] Before sampling - pos: {len(pos_pair_list)} - neg: {len(neg_pair_list)}\"\n",
    "    )\n",
    "\n",
    "    pos_pair_list = [[x] for x in pos_pair_list]\n",
    "    neg_pair_list = [neg_pair_list[i*num_negatives:(i+1)*num_negatives] for i in range(len(neg_pair_list) // num_negatives)]\n",
    "    \n",
    "    assert len(pos_pair_list) == len(neg_pair_list)\n",
    "    \n",
    "    # Sub sample the positive and negative pairs to num_pairs\n",
    "    if len(pos_pair_list) > num_pairs:\n",
    "        sampled_list = random.sample(list(range(len(pos_pair_list))), num_pairs)\n",
    "        pos_pair_list = [pos_pair_list[x] for x in sampled_list]\n",
    "        neg_pair_list = [neg_pair_list[x] for x in sampled_list]\n",
    "        \n",
    "        pos_pair_list = [x[0] for x in pos_pair_list]\n",
    "        neg_pair_list = list(itertools.chain(*neg_pair_list))\n",
    "        \n",
    "        print(\n",
    "            f\"[D] After sampling - pos: {len(pos_pair_list)} - neg: {len(neg_pair_list)}\"\n",
    "        )\n",
    "\n",
    "    return pos_pair_list, neg_pair_list\n",
    "\n",
    "    \n",
    "\n",
    "# %%\n",
    "def convert_dicts_into_dataframes(df_input, dataset_dict):\n",
    "    pair_columns = [\n",
    "        \"idb_path_1\",\n",
    "        \"fva_1\",\n",
    "        \"func_name_1\",\n",
    "        \"idb_path_2\",\n",
    "        \"fva_2\",\n",
    "        \"func_name_2\",\n",
    "        \"db_type\",\n",
    "    ]\n",
    "\n",
    "    pos_pair_dict = defaultdict(list)\n",
    "    neg_pair_dict = defaultdict(list)\n",
    "    \n",
    "    # Iterate over each positive and negative pair.\n",
    "    #   Select the required info and save it in a new dictionary.\n",
    "    for task in dataset_dict:\n",
    "        for pos_pair in dataset_dict[task][\"pos\"]:\n",
    "            while len(pos_pair) == 1:\n",
    "                pos_pair = pos_pair[0]\n",
    "            for c in [\"idb_path\", \"fva\", \"func_name\"]:\n",
    "                pos_pair_dict[c + \"_1\"].append(df_input.iloc[pos_pair[0]][c])\n",
    "                pos_pair_dict[c + \"_2\"].append(df_input.iloc[pos_pair[1]][c])\n",
    "            pos_pair_dict[\"db_type\"].append(task)\n",
    "\n",
    "        for neg_pair in dataset_dict[task][\"neg\"]:\n",
    "            while len(neg_pair) == 1:\n",
    "                neg_pair = neg_pair[0]\n",
    "            for c in [\"idb_path\", \"fva\", \"func_name\"]:\n",
    "                neg_pair_dict[c + \"_1\"].append(df_input.iloc[neg_pair[0]][c])\n",
    "                neg_pair_dict[c + \"_2\"].append(df_input.iloc[neg_pair[1]][c])\n",
    "            neg_pair_dict[\"db_type\"].append(task)\n",
    "    \n",
    "    # Convert the local pair_dicts into DataFrames\n",
    "    df_pos = pd.DataFrame.from_dict(pos_pair_dict)\n",
    "    df_neg = pd.DataFrame.from_dict(neg_pair_dict)\n",
    "    \n",
    "    # Check/change the order of the columns\n",
    "    df_pos = df_pos[pair_columns]\n",
    "    df_neg = df_neg[pair_columns]\n",
    "    return df_pos, df_neg\n",
    "\n",
    "# %%\n",
    "def print_summary(dataset_dict):\n",
    "    print(\"[D] Summary:\") \n",
    "    for task in dataset_dict:\n",
    "        print(\n",
    "            \"[D] \\tTask: {:5} - pos: {:5} neg: {:5}\".format(\n",
    "                task, len(dataset_dict[task][\"pos\"]), len(dataset_dict[task][\"neg\"])\n",
    "            )\n",
    "        )\n",
    "    print(\"\\n\")\n",
    "\n",
    "# %%\n",
    "def print_free_variables(df_input, task_list, dataset_dict):\n",
    "    for task in task_list:\n",
    "        # Skip \"XM\"\n",
    "        if task not in TASKS_DICT:\n",
    "            continue\n",
    "\n",
    "        print(\"-\" * 100 + \"\\n\")\n",
    "        print(f\"[D] Task: {task}\\n\")\n",
    "\n",
    "        # Get the name of the free variables for each task\n",
    "        free_variables = list(\n",
    "            compress(CATEGORIES, [not x for x in TASKS_DICT[task]])\n",
    "        )\n",
    "\n",
    "        v_list = list()\n",
    "        for pos_pair in dataset_dict[task][\"pos\"]:\n",
    "            while len(pos_pair) == 1:\n",
    "                pos_pair = pos_pair[0]\n",
    "            # Get the values associated to the free variables\n",
    "            vv = df_input.iloc[list(pos_pair)][free_variables].values\n",
    "            # Sort them to avoid counting the permutations\n",
    "            vv = tuple(sorted([tuple(x) for x in vv]))\n",
    "            v_list.append(vv)\n",
    "\n",
    "        # Print the frequency of each combination\n",
    "        for k, v in Counter(v_list).most_common():\n",
    "            print(f\"\\t{v:5}, {k}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "# %%\n",
    "\n",
    "def create_pos_neg_dataset(\n",
    "    df_input, task_dict, output_dir, output_fs, rand=True, num_negatives=1\n",
    "):\n",
    "    print(\"[D] Creating the pos/neg function pairs...\", flush=True)\n",
    "    dataset_dict = defaultdict(dict)\n",
    "\n",
    "    for task, num_pairs in task_dict.items():\n",
    "        ppl, npl = None, None\n",
    "        if rand:\n",
    "            # Use the random version of the pair generation function\n",
    "            ppl, npl = create_similarity_pairs_random_version_parallel(\n",
    "                df_input, num_pairs, task, num_negatives\n",
    "            )\n",
    "        else:\n",
    "            ppl, npl = create_similarity_pairs_parallel(df_input, num_pairs, task)\n",
    "        dataset_dict[task][\"pos\"] = ppl\n",
    "        dataset_dict[task][\"neg\"] = npl\n",
    "\n",
    "    print_summary(dataset_dict)\n",
    "\n",
    "    print(\"[D] Converting the positive/negative pairs into CSV...\", flush=True)\n",
    "    df_pos, df_neg = convert_dicts_into_dataframes(df_input, dataset_dict)\n",
    "\n",
    "    pos_fp = os.path.join(output_dir, output_fs.format(\"pos\"))\n",
    "    df_pos.to_csv(pos_fp)\n",
    "    print(f\"[D] \\tPos CSV: {pos_fp}\")\n",
    "\n",
    "    neg_fp = os.path.join(output_dir, output_fs.format(\"neg\"))\n",
    "    df_neg.to_csv(neg_fp)\n",
    "    print(f\"[D] \\tNeg CSV: {neg_fp}\")\n",
    "\n",
    "    # For debug only\n",
    "    print_free_variables(df_input, task_dict.keys(), dataset_dict)\n",
    "    \n",
    "    selected_functions = set()\n",
    "    for task in dataset_dict:\n",
    "        for pair in dataset_dict[task][\"pos\"]:\n",
    "            while len(pair) == 1:\n",
    "                pair = pair[0]\n",
    "            selected_functions.update(list(pair))\n",
    "        for pair in dataset_dict[task][\"neg\"]:\n",
    "            while len(pair) == 1:\n",
    "                pair = pair[0]\n",
    "            selected_functions.update(list(pair))\n",
    "    return selected_functions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ed95d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Creating the pos/neg function pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All Function consumed:  27%|       | 3489/12729 [00:02<00:07, 1170.46it/s]\n",
      "Collected Pairs: 100%|| 12729/12729 [00:03<00:00, 4069.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Before sampling - pos: 12729 - neg: 12729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collected Pairs: 12733it [00:01, 8597.53it/s]                            t/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Before sampling - pos: 12733 - neg: 12733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collected Pairs: 12733it [00:01, 8356.14it/s]                            t/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Before sampling - pos: 12733 - neg: 12733\n",
      "[D] Summary:\n",
      "[D] \tTask: XA    - pos: 12729 neg: 12729\n",
      "[D] \tTask: XA+XO - pos: 12733 neg: 12733\n",
      "[D] \tTask: XO    - pos: 12733 neg: 12733\n",
      "\n",
      "\n",
      "[D] Converting the positive/negative pairs into CSV...\n",
      "[D] \tPos CSV: ../Dataset-2/pairs/_testing_Dataset-2.csv\n",
      "[D] \tNeg CSV: ../Dataset-2/pairs/_testing_Dataset-2.csv\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[D] Task: XA\n",
      "\n",
      "\t 6403, (('arm', '32'), ('x86', '64'))\n",
      "\t 6047, (('mips', '32'), ('x86', '64'))\n",
      "\t  208, (('arm', '64'), ('mips', '32'))\n",
      "\t   71, (('arm', '64'), ('x86', '32'))\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[D] Task: XA+XO\n",
      "\n",
      "\t  960, (('arm', '32', 'O3'), ('x86', '64', 'O1'))\n",
      "\t  856, (('mips', '32', 'O1'), ('x86', '64', 'O3'))\n",
      "\t  819, (('arm', '32', 'O3'), ('x86', '64', 'O0'))\n",
      "\t  800, (('mips', '32', 'O2'), ('x86', '64', 'O1'))\n",
      "\t  800, (('arm', '32', 'O1'), ('x86', '64', 'O3'))\n",
      "\t  797, (('arm', '32', 'O1'), ('x86', '64', 'O0'))\n",
      "\t  790, (('mips', '32', 'O2'), ('x86', '64', 'O3'))\n",
      "\t  786, (('mips', '32', 'O1'), ('x86', '64', 'O0'))\n",
      "\t  682, (('mips', '32', 'O2'), ('x86', '64', 'O0'))\n",
      "\t  678, (('mips', '32', 'O0'), ('x86', '64', 'O1'))\n",
      "\t  579, (('mips', '32', 'O0'), ('x86', '64', 'O3'))\n",
      "\t  577, (('arm', '32', 'O3'), ('x86', '64', 'O2'))\n",
      "\t  533, (('mips', '32', 'O3'), ('x86', '64', 'O1'))\n",
      "\t  510, (('mips', '32', 'O1'), ('x86', '64', 'O2'))\n",
      "\t  461, (('mips', '32', 'O3'), ('x86', '64', 'O0'))\n",
      "\t  446, (('arm', '32', 'O1'), ('x86', '64', 'O2'))\n",
      "\t  361, (('mips', '32', 'O3'), ('x86', '64', 'O2'))\n",
      "\t  329, (('mips', '32', 'O0'), ('x86', '64', 'O2'))\n",
      "\t  286, (('arm', '32', 'O0'), ('x86', '64', 'O1'))\n",
      "\t  246, (('arm', '32', 'O0'), ('x86', '64', 'O3'))\n",
      "\t  176, (('arm', '32', 'O0'), ('x86', '64', 'O2'))\n",
      "\t   35, (('mips', '32', 'O0'), ('x86', '64', 'cffobf'))\n",
      "\t   30, (('mips', '32', 'O0'), ('x86', '64', 'subobf'))\n",
      "\t   23, (('mips', '32', 'O0'), ('x86', '64', 'splitobf'))\n",
      "\t   22, (('mips', '32', 'O0'), ('x86', '64', 'bcfobf'))\n",
      "\t   12, (('mips', '32', 'O3'), ('x86', '64', 'cffobf'))\n",
      "\t   11, (('mips', '32', 'O1'), ('x86', '64', 'cffobf'))\n",
      "\t   11, (('mips', '32', 'O1'), ('x86', '64', 'subobf'))\n",
      "\t   10, (('mips', '32', 'O3'), ('x86', '64', 'subobf'))\n",
      "\t    9, (('arm', '32', 'O2'), ('x86', '64', 'O1'))\n",
      "\t    9, (('arm', '32', 'O2'), ('x86', '64', 'cffobf'))\n",
      "\t    8, (('mips', '32', 'O2'), ('x86', '64', 'cffobf'))\n",
      "\t    7, (('mips', '32', 'O3'), ('x86', '64', 'bcfobf'))\n",
      "\t    6, (('arm', '32', 'O2'), ('x86', '64', 'bcfobf'))\n",
      "\t    6, (('arm', '32', 'O0'), ('x86', '64', 'cffobf'))\n",
      "\t    6, (('mips', '32', 'O1'), ('x86', '64', 'bcfobf'))\n",
      "\t    6, (('mips', '32', 'O1'), ('x86', '64', 'splitobf'))\n",
      "\t    6, (('mips', '32', 'O3'), ('x86', '64', 'splitobf'))\n",
      "\t    6, (('mips', '32', 'O2'), ('x86', '64', 'subobf'))\n",
      "\t    5, (('arm', '32', 'O0'), ('x86', '64', 'bcfobf'))\n",
      "\t    5, (('mips', '32', 'O2'), ('x86', '64', 'bcfobf'))\n",
      "\t    3, (('arm', '32', 'O1'), ('x86', '64', 'cffobf'))\n",
      "\t    3, (('arm', '32', 'O2'), ('x86', '64', 'splitobf'))\n",
      "\t    3, (('mips', '32', 'O2'), ('x86', '64', 'splitobf'))\n",
      "\t    3, (('arm', '32', 'O0'), ('x86', '64', 'subobf'))\n",
      "\t    3, (('arm', '32', 'O0'), ('x86', '64', 'splitobf'))\n",
      "\t    2, (('arm', '32', 'O2'), ('x86', '64', 'O0'))\n",
      "\t    2, (('arm', '32', 'O2'), ('x86', '64', 'subobf'))\n",
      "\t    2, (('arm', '32', 'O1'), ('x86', '64', 'subobf'))\n",
      "\t    2, (('arm', '32', 'O1'), ('x86', '64', 'splitobf'))\n",
      "\t    1, (('arm', '32', 'O2'), ('x86', '64', 'O3'))\n",
      "\t    1, (('arm', '32', 'O1'), ('x86', '64', 'bcfobf'))\n",
      "\t    1, (('arm', '32', 'O3'), ('x86', '64', 'bcfobf'))\n",
      "\t    1, (('arm', '32', 'O3'), ('x86', '64', 'subobf'))\n",
      "\t    1, (('arm', '32', 'O3'), ('x86', '64', 'splitobf'))\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[D] Task: XO\n",
      "\n",
      "\t 2725, (('O1',), ('O3',))\n",
      "\t 2215, (('O0',), ('O1',))\n",
      "\t 1740, (('O0',), ('O3',))\n",
      "\t 1727, (('O1',), ('O2',))\n",
      "\t 1434, (('O2',), ('O3',))\n",
      "\t 1295, (('O0',), ('O2',))\n",
      "\t   91, (('splitobf',), ('subobf',))\n",
      "\t   84, (('O1',), ('bcfobf',))\n",
      "\t   83, (('O0',), ('splitobf',))\n",
      "\t   83, (('O0',), ('bcfobf',))\n",
      "\t   83, (('O1',), ('subobf',))\n",
      "\t   80, (('O1',), ('splitobf',))\n",
      "\t   80, (('bcfobf',), ('subobf',))\n",
      "\t   80, (('O0',), ('subobf',))\n",
      "\t   77, (('O3',), ('bcfobf',))\n",
      "\t   75, (('bcfobf',), ('splitobf',))\n",
      "\t   74, (('O2',), ('subobf',))\n",
      "\t   73, (('O3',), ('splitobf',))\n",
      "\t   73, (('O2',), ('splitobf',))\n",
      "\t   69, (('O3',), ('subobf',))\n",
      "\t   68, (('O2',), ('bcfobf',))\n",
      "\t   66, (('O1',), ('cffobf',))\n",
      "\t   63, (('cffobf',), ('subobf',))\n",
      "\t   58, (('O2',), ('cffobf',))\n",
      "\t   56, (('bcfobf',), ('cffobf',))\n",
      "\t   55, (('O0',), ('cffobf',))\n",
      "\t   53, (('cffobf',), ('splitobf',))\n",
      "\t   44, (('O3',), ('cffobf',))\n",
      "\t    5, (('indibran',), ('splitobf',))\n",
      "\t    5, (('bcfobf',), ('indibran',))\n",
      "\t    4, (('O3',), ('indibran',))\n",
      "\t    4, (('indibran',), ('subobf',))\n",
      "\t    4, (('O1',), ('indibran',))\n",
      "\t    3, (('O2',), ('indibran',))\n",
      "\t    3, (('O0',), ('indibran',))\n",
      "\t    1, (('cffobf',), ('indibran',))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create pairs for test dataset\n",
    "sf_set_1 = create_pos_neg_dataset(\n",
    "    df,\n",
    "    DATASET_TWO_DICT[\"eval\"][\"test\"][\"similarity\"],\n",
    "    os.path.join(OUTPUT_DIR, \"pairs\"),\n",
    "    \"_testing_Dataset-2.csv\",\n",
    "    rand=False,\n",
    "    num_negatives=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae9598dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Creating the pos/neg function pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Pairs: 100%|| 300/300 [00:00<00:00, 766.10it/s]68it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Before sampling - pos: 300 - neg: 30000\n",
      "[D] After sampling - pos: 200 - neg: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Pairs: 307it [00:00, 641.67it/s]                         67it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Before sampling - pos: 307 - neg: 30700\n",
      "[D] After sampling - pos: 200 - neg: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Pairs: 313it [00:00, 766.41it/s]                         75it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Before sampling - pos: 313 - neg: 31300\n",
      "[D] After sampling - pos: 200 - neg: 20000\n",
      "[D] Summary:\n",
      "[D] \tTask: XA    - pos:   200 neg: 20000\n",
      "[D] \tTask: XA+XO - pos:   200 neg: 20000\n",
      "[D] \tTask: XO    - pos:   200 neg: 20000\n",
      "\n",
      "\n",
      "[D] Converting the positive/negative pairs into CSV...\n",
      "[D] \tPos CSV: ../Dataset-2/pairs/_rank_testing_Dataset-2.csv\n",
      "[D] \tNeg CSV: ../Dataset-2/pairs/_rank_testing_Dataset-2.csv\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[D] Task: XA\n",
      "\n",
      "\t  109, (('arm', '32'), ('x86', '64'))\n",
      "\t   70, (('mips', '32'), ('x86', '64'))\n",
      "\t   15, (('arm', '64'), ('mips', '32'))\n",
      "\t    6, (('arm', '64'), ('x86', '32'))\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[D] Task: XA+XO\n",
      "\n",
      "\t   10, (('arm', '32', 'O0'), ('x86', '64', 'splitobf'))\n",
      "\t   10, (('arm', '32', 'O0'), ('x86', '64', 'cffobf'))\n",
      "\t    7, (('mips', '32', 'O0'), ('x86', '64', 'bcfobf'))\n",
      "\t    7, (('mips', '32', 'O0'), ('x86', '64', 'O3'))\n",
      "\t    6, (('mips', '32', 'O2'), ('x86', '64', 'O3'))\n",
      "\t    5, (('mips', '32', 'O1'), ('x86', '64', 'O0'))\n",
      "\t    5, (('mips', '32', 'O3'), ('x86', '64', 'O1'))\n",
      "\t    5, (('arm', '32', 'O1'), ('x86', '64', 'cffobf'))\n",
      "\t    5, (('mips', '32', 'O3'), ('x86', '64', 'O2'))\n",
      "\t    4, (('arm', '32', 'O0'), ('x86', '64', 'subobf'))\n",
      "\t    4, (('arm', '32', 'O1'), ('x86', '64', 'splitobf'))\n",
      "\t    4, (('arm', '32', 'O3'), ('x86', '64', 'O2'))\n",
      "\t    4, (('arm', '32', 'O3'), ('x86', '64', 'cffobf'))\n",
      "\t    4, (('arm', '32', 'O0'), ('x86', '64', 'bcfobf'))\n",
      "\t    4, (('mips', '32', 'O2'), ('x86', '64', 'O1'))\n",
      "\t    4, (('mips', '32', 'O1'), ('x86', '64', 'O3'))\n",
      "\t    4, (('arm', '32', 'O1'), ('x86', '64', 'subobf'))\n",
      "\t    4, (('arm', '32', 'O3'), ('x86', '64', 'O1'))\n",
      "\t    4, (('mips', '32', 'O1'), ('x86', '64', 'bcfobf'))\n",
      "\t    4, (('mips', '32', 'O0'), ('x86', '64', 'cffobf'))\n",
      "\t    3, (('arm', '32', 'O3'), ('x86', '64', 'O0'))\n",
      "\t    3, (('mips', '32', 'O1'), ('x86', '64', 'splitobf'))\n",
      "\t    3, (('mips', '32', 'O0'), ('x86', '64', 'subobf'))\n",
      "\t    3, (('arm', '32', 'O2'), ('x86', '64', 'bcfobf'))\n",
      "\t    3, (('arm', '32', 'O0'), ('x86', '64', 'O1'))\n",
      "\t    3, (('arm', '32', 'O0'), ('x86', '64', 'O3'))\n",
      "\t    3, (('mips', '32', 'O2'), ('x86', '64', 'O0'))\n",
      "\t    3, (('arm', '32', 'O1'), ('x86', '64', 'O3'))\n",
      "\t    3, (('arm', '32', 'O1'), ('x86', '64', 'bcfobf'))\n",
      "\t    3, (('mips', '32', 'O0'), ('x86', '64', 'funcwra'))\n",
      "\t    3, (('mips', '32', 'O0'), ('x86', '64', 'splitobf'))\n",
      "\t    3, (('arm', '32', 'O2'), ('x86', '64', 'subobf'))\n",
      "\t    3, (('mips', '32', 'O1'), ('x86', '64', 'cffobf'))\n",
      "\t    3, (('mips', '32', 'O3'), ('x86', '64', 'splitobf'))\n",
      "\t    3, (('arm', '32', 'O2'), ('x86', '64', 'splitobf'))\n",
      "\t    3, (('mips', '32', 'O1'), ('x86', '64', 'O2'))\n",
      "\t    3, (('mips', '32', 'O2'), ('x86', '64', 'splitobf'))\n",
      "\t    3, (('mips', '32', 'O0'), ('x86', '64', 'O1'))\n",
      "\t    2, (('arm', '32', 'O3'), ('x86', '64', 'splitobf'))\n",
      "\t    2, (('mips', '32', 'O0'), ('x86', '64', 'O2'))\n",
      "\t    2, (('arm', '32', 'O3'), ('x86', '64', 'bcfobf'))\n",
      "\t    2, (('arm', '32', 'O1'), ('x86', '64', 'O2'))\n",
      "\t    2, (('arm', '32', 'O3'), ('x86', '64', 'subobf'))\n",
      "\t    2, (('arm', '32', 'O0'), ('x86', '64', 'O2'))\n",
      "\t    2, (('arm', '32', 'O2'), ('x86', '64', 'O1'))\n",
      "\t    2, (('mips', '32', 'O3'), ('x86', '64', 'O0'))\n",
      "\t    2, (('arm', '32', 'O2'), ('x86', '64', 'cffobf'))\n",
      "\t    1, (('arm', '64', 'O1'), ('mips', '32', 'O2'))\n",
      "\t    1, (('arm', '32', 'O0'), ('x86', '64', 'funcwra'))\n",
      "\t    1, (('mips', '32', 'O2'), ('x86', '64', 'indibran'))\n",
      "\t    1, (('mips', '32', 'O3'), ('x86', '64', 'cffobf'))\n",
      "\t    1, (('mips', '32', 'O3'), ('x86', '64', 'bcfobf'))\n",
      "\t    1, (('arm', '64', 'O2'), ('mips', '32', 'O1'))\n",
      "\t    1, (('arm', '32', 'O2'), ('x86', '64', 'indibran'))\n",
      "\t    1, (('arm', '32', 'O1'), ('x86', '64', 'indibran'))\n",
      "\t    1, (('arm', '32', 'O0'), ('x86', '64', 'indibran'))\n",
      "\t    1, (('mips', '32', 'O3'), ('x86', '64', 'indibran'))\n",
      "\t    1, (('mips', '32', 'O1'), ('x86', '64', 'indibran'))\n",
      "\t    1, (('arm', '64', 'O0'), ('mips', '32', 'O3'))\n",
      "\t    1, (('arm', '32', 'O2'), ('x86', '64', 'O3'))\n",
      "\t    1, (('arm', '64', 'O0'), ('x86', '32', 'splitobf'))\n",
      "\t    1, (('mips', '32', 'O2'), ('x86', '64', 'subobf'))\n",
      "\t    1, (('arm', '32', 'O1'), ('x86', '64', 'O0'))\n",
      "\t    1, (('mips', '32', 'O2'), ('x86', '64', 'bcfobf'))\n",
      "\t    1, (('mips', '32', 'O2'), ('x86', '64', 'cffobf'))\n",
      "\t    1, (('arm', '64', 'O1'), ('mips', '32', 'O0'))\n",
      "\t    1, (('arm', '64', 'O3'), ('mips', '32', 'O0'))\n",
      "\t    1, (('arm', '64', 'O2'), ('mips', '32', 'O3'))\n",
      "\t    1, (('arm', '32', 'O2'), ('x86', '64', 'O0'))\n",
      "\t    1, (('mips', '32', 'O1'), ('x86', '64', 'subobf'))\n",
      "\t    1, (('arm', '32', 'O2'), ('x86', '64', 'funcwra'))\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[D] Task: XO\n",
      "\n",
      "\t   29, (('O0',), ('O1',))\n",
      "\t   22, (('O0',), ('O2',))\n",
      "\t   18, (('O1',), ('O3',))\n",
      "\t   16, (('O1',), ('O2',))\n",
      "\t   15, (('O0',), ('O3',))\n",
      "\t   14, (('O2',), ('O3',))\n",
      "\t   10, (('bcfobf',), ('splitobf',))\n",
      "\t    6, (('cffobf',), ('subobf',))\n",
      "\t    6, (('bcfobf',), ('cffobf',))\n",
      "\t    5, (('bcfobf',), ('subobf',))\n",
      "\t    5, (('O1',), ('subobf',))\n",
      "\t    5, (('O0',), ('splitobf',))\n",
      "\t    4, (('O0',), ('bcfobf',))\n",
      "\t    4, (('splitobf',), ('subobf',))\n",
      "\t    4, (('O0',), ('cffobf',))\n",
      "\t    3, (('O1',), ('splitobf',))\n",
      "\t    3, (('O1',), ('bcfobf',))\n",
      "\t    3, (('cffobf',), ('splitobf',))\n",
      "\t    3, (('O2',), ('bcfobf',))\n",
      "\t    3, (('bcfobf',), ('funcwra',))\n",
      "\t    3, (('O0',), ('subobf',))\n",
      "\t    3, (('O2',), ('cffobf',))\n",
      "\t    2, (('O2',), ('splitobf',))\n",
      "\t    2, (('funcwra',), ('splitobf',))\n",
      "\t    2, (('O0',), ('funcwra',))\n",
      "\t    2, (('bcfobf',), ('strcry',))\n",
      "\t    1, (('O3',), ('bcfobf',))\n",
      "\t    1, (('cffobf',), ('funcwra',))\n",
      "\t    1, (('splitobf',), ('strcry',))\n",
      "\t    1, (('O0',), ('strcry',))\n",
      "\t    1, (('funcwra',), ('subobf',))\n",
      "\t    1, (('cffobf',), ('strcry',))\n",
      "\t    1, (('funcwra',), ('strcry',))\n",
      "\t    1, (('O1',), ('cffobf',))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create pairs for test rank dataset\n",
    "sf_set_2 = create_pos_neg_dataset(\n",
    "    df,\n",
    "    DATASET_TWO_DICT[\"eval\"][\"test\"][\"rank\"],\n",
    "    os.path.join(OUTPUT_DIR, \"pairs\"),\n",
    "    \"_rank_testing_Dataset-2.csv\",\n",
    "    rand=True,\n",
    "    num_negatives=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "febe694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[list(sf_set_1 | sf_set_2)]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba69269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape df: \t\t(76817, 14)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape df: \\t\\t{df.shape}\")\n",
    "\n",
    "# Save the \"selected functions\" to a CSV.\n",
    "# This will be useful to post-process the results.\n",
    "df.to_csv(os.path.join(OUTPUT_DIR, \"testing_Dataset-2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3354834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 76817 functions\n"
     ]
    }
   ],
   "source": [
    "# Save the \"selected functions\" to a JSON.\n",
    "# This is useful to limit the IDA analysis to some functions only.\n",
    "fset = set([tuple(x) for x in df[['idb_path', 'fva']].values])\n",
    "print(\"Testing: {} functions\".format(len(fset)))\n",
    "\n",
    "selected_functions = defaultdict(list)\n",
    "for t in fset:\n",
    "    selected_functions[t[0]].append(int(t[1], 16))\n",
    "    \n",
    "# Test\n",
    "assert(sum([len(v) for v in selected_functions.values()]) == len(fset))\n",
    "\n",
    "# Save to file\n",
    "with open(os.path.join(OUTPUT_DIR, \"features\", \"selected_testing_Dataset-2.json\"), \"w\") as f_out:\n",
    "    json.dump(selected_functions, f_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
